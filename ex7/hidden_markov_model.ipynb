{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7.1: The Forward-Backward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the HMM parameters\n",
    "states = [1, 2, 3, 4, 5]\n",
    "vocab = [\n",
    "    \"a\",\n",
    "    \"the\",\n",
    "    \"over\",\n",
    "    \"beside\",\n",
    "    \"near\",\n",
    "    \"quick\",\n",
    "    \"brown\",\n",
    "    \"lazy\",\n",
    "    \"jumps\",\n",
    "    \"runs\",\n",
    "    \"walks\",\n",
    "    \"fox\",\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "]\n",
    "beta = [\n",
    "    [0.6, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0.2, 0.4, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0.5, 0.3, 0.2, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.4, 0.5, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0.4, 0.3],\n",
    "]\n",
    "theta = [\n",
    "    [0, 0, 0.5, 0, 0.5],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 0.3, 0, 0.7],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0.5, 0, 0.5, 0],\n",
    "]\n",
    "initial_probs = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 11, 8, 2, 0, 12]\n"
     ]
    }
   ],
   "source": [
    "# Define the observation sequence\n",
    "obs_seq = ['the', 'quick', 'fox', 'jumps', 'over', 'a', 'dog']\n",
    "obs_seq_indices = [vocab.index(obs) for obs in obs_seq]\n",
    "print(obs_seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Initialize the forward and backward variables\n",
    "N = len(states)\n",
    "T = len(obs_seq)\n",
    "print(N)\n",
    "print(T)\n",
    "# Initialize the chosen variables\n",
    "t = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_algorithm(beta, theta, initial_probs, states, obs_seq_indices):\n",
    "    # Initialize the forward and backward variables\n",
    "    N = len(states)\n",
    "    T = len(obs_seq_indices)\n",
    "    forward = np.zeros((N, T))\n",
    "    backward = np.zeros((N, T))\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(T):\n",
    "        for i in range(N):\n",
    "            if t == 0:\n",
    "                forward[i, t] = initial_probs[i] * beta[i][obs_seq_indices[t]]\n",
    "            else:\n",
    "                forward[i, t] = beta[i][obs_seq_indices[t]] * np.sum(\n",
    "                    [forward[j, t - 1] * theta[j][i] for j in range(N)]\n",
    "                )\n",
    "\n",
    "    # Backward pass\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for i in range(N):\n",
    "            if t == T - 1:\n",
    "                backward[i, t] = 1\n",
    "            else:\n",
    "                backward[i, t] = np.sum(\n",
    "                    [theta[i][j] * beta[j][obs_seq_indices[t + 1]] * backward[j, t + 1] for j in range(N)]\n",
    "                )\n",
    "\n",
    "    # Calculate the probability\n",
    "    probability = np.sum([forward[i, T - 1] * backward[i, T - 1] for i in range(N)])\n",
    "\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.040000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probability\n",
    "probability = forward_backward_algorithm(beta, theta, initial_probs, states, obs_seq_indices)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7.2: The Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11, 3, 4, 9, 0, 10, 13]\n"
     ]
    }
   ],
   "source": [
    "# Define the HMM parameters\n",
    "states = [1, 2, 3, 4, 5]\n",
    "vocab = [\n",
    "    \"a\",\n",
    "    \"the\",\n",
    "    \"I\",\n",
    "    \"you\",\n",
    "    \"can\",\n",
    "    \"will\",\n",
    "    \"call\",\n",
    "    \"own\",\n",
    "    \"take\",\n",
    "    \"book\",\n",
    "    \"round\",\n",
    "    \"claim\",\n",
    "    \"car\",\n",
    "    \"hotel\",\n",
    "    \"new\",\n",
    "    \"great\",\n",
    "]\n",
    "beta = [\n",
    "    [0.4, 0.3, 0.2, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0.3, 0.4, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0.1, 0.15, 0.15, 0.15, 0.15, 0.1, 0.1, 0.1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0.4, 0.3],\n",
    "    [0, 0, 0, 0.05, 0.05, 0.05, 0.1, 0, 0, 0.15, 0.1, 0.15, 0.2, 0.15, 0, 0],\n",
    "]\n",
    "theta = [\n",
    "    [0, 0, 0.3, 0.3, 0.4],\n",
    "    [0, 0, 0, 0.4, 0.6],\n",
    "    [0, 0.75, 0.25, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 0],\n",
    "]\n",
    "initial_probs = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform distribution\n",
    "\n",
    "# Define the observation sequence\n",
    "obs_seq = [\"you\", \"claim\", \"you\", \"can\", \"book\", \"a\", \"round\", \"hotel\"]\n",
    "obs_seq_indices = [vocab.index(obs) for obs in obs_seq]\n",
    "print(obs_seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Viterbi variables\n",
    "N = len(states)\n",
    "T = len(obs_seq)\n",
    "viterbi = np.zeros((N, T))\n",
    "backpointer = np.zeros((N, T), dtype=int)\n",
    "\n",
    "# Initialization\n",
    "for s in range(N):\n",
    "    viterbi[s, 0] = initial_probs[s] * beta[s][obs_seq_indices[0]]\n",
    "print(N)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best path: [4, 2, 1, 4, 2, 1, 3, 4]\n",
      "Best path probability: 2.7337500000000004e-09\n"
     ]
    }
   ],
   "source": [
    "# Recursion\n",
    "for t in range(1, T):\n",
    "    for s in range(N):\n",
    "        viterbi[s, t] = max(\n",
    "            [\n",
    "                viterbi[s_prime, t - 1]\n",
    "                * theta[s_prime][s]\n",
    "                * beta[s][obs_seq_indices[t]]\n",
    "                for s_prime in range(N)\n",
    "            ]\n",
    "        )\n",
    "        backpointer[s, t] = np.argmax(\n",
    "            [viterbi[s_prime, t - 1] * theta[s_prime][s] for s_prime in range(N)]\n",
    "        )\n",
    "\n",
    "# Termination\n",
    "best_path_prob = np.max(viterbi[:, T - 1])\n",
    "best_path_pointer = np.argmax(viterbi[:, T - 1])\n",
    "\n",
    "# Backtrack to get the best path\n",
    "best_path = []\n",
    "for t in range(T - 1, -1, -1):\n",
    "    best_path.insert(0, best_path_pointer)\n",
    "    best_path_pointer = backpointer[best_path_pointer, t]\n",
    "\n",
    "print(\"Best path:\", best_path)\n",
    "print(\"Best path probability:\", best_path_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7.3: The Baum-Welch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated transition matrix: [[0.00e+00 0.00e+00 3.15e-05 0.00e+00 1.00e-01]\n",
      " [1.20e-01 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 2.52e-04]\n",
      " [0.00e+00 2.40e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 1.20e-03 0.00e+00]]\n",
      "Updated emission matrix: [[0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HMM parameters from the problem\n",
    "initial_distribution = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "transition_matrix = np.array(\n",
    "    [\n",
    "        [0, 0, 0.5, 0, 0.5],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [0, 0, 0.3, 0, 0.7],\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 0.5, 0, 0.5, 0],\n",
    "    ]\n",
    ")\n",
    "emission_matrix = np.array(\n",
    "    [\n",
    "        [0.6, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0.2, 0.4, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0.5, 0.3, 0.2, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.4, 0.5, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0.4, 0.3],\n",
    "    ]\n",
    ")\n",
    "word_to_index = {\n",
    "    word: index\n",
    "    for index, word in enumerate(\n",
    "        [\n",
    "            \"a\",\n",
    "            \"the\",\n",
    "            \"over\",\n",
    "            \"beside\",\n",
    "            \"near\",\n",
    "            \"quick\",\n",
    "            \"brown\",\n",
    "            \"lazy\",\n",
    "            \"jumps\",\n",
    "            \"runs\",\n",
    "            \"walks\",\n",
    "            \"fox\",\n",
    "            \"dog\",\n",
    "            \"cat\",\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "observed_sequence = [\n",
    "    word_to_index[word] for word in \"the quick fox jumps over a dog\".split()\n",
    "]\n",
    "\n",
    "N = initial_distribution.shape[0]\n",
    "T = len(observed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the forward probabilities matrix\n",
    "forward_probabilities = np.zeros((N, T))\n",
    "\n",
    "# Initialize the forward probabilities for the first observation\n",
    "for i in range(N):\n",
    "    forward_probabilities[i, 0] = (\n",
    "        initial_distribution[i] * emission_matrix[i, observed_sequence[0]]\n",
    "    )\n",
    "\n",
    "# Compute the forward probabilities for the rest of the observations\n",
    "for t in range(1, T):\n",
    "    for j in range(N):\n",
    "        forward_probabilities[j, t] = (\n",
    "            np.sum(forward_probabilities[:, t - 1] * transition_matrix[:, j])\n",
    "            * emission_matrix[j, observed_sequence[t]]\n",
    "        )\n",
    "# Initialize the backward probabilities matrix\n",
    "backward_probabilities = np.zeros((N, T))\n",
    "\n",
    "# Initialize the backward probabilities for the last observation\n",
    "backward_probabilities[:, -1] = 1\n",
    "\n",
    "# Compute the backward probabilities for the rest of the observations\n",
    "for t in range(T - 2, -1, -1):\n",
    "    for i in range(N):\n",
    "        backward_probabilities[i, t] = np.sum(\n",
    "            backward_probabilities[:, t + 1]\n",
    "            * transition_matrix[i, :]\n",
    "            * emission_matrix[:, observed_sequence[t + 1]]\n",
    "        )\n",
    "# Compute the posterior probabilities\n",
    "posterior_probabilities = (\n",
    "    forward_probabilities\n",
    "    * backward_probabilities\n",
    "    / np.sum(forward_probabilities[:, -1])\n",
    ")\n",
    "\n",
    "# Initialize the new transition matrix and emission matrix\n",
    "new_transition_matrix = np.zeros((N, N))\n",
    "new_emission_matrix = np.zeros((N, len(word_to_index)))\n",
    "\n",
    "# Update the transition matrix and emission matrix\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        new_transition_matrix[i, j] = np.sum(\n",
    "            posterior_probabilities[i, :-1]\n",
    "            * transition_matrix[i, j]\n",
    "            * emission_matrix[j, observed_sequence[1:]]\n",
    "            * backward_probabilities[j, 1:]\n",
    "        ) / np.sum(posterior_probabilities[i, :-1])\n",
    "    for w in range(len(word_to_index)):\n",
    "        new_emission_matrix[i, w] = np.sum(\n",
    "            posterior_probabilities[i, np.array(observed_sequence) == w]\n",
    "        ) / np.sum(posterior_probabilities[i, :])\n",
    "\n",
    "# The new transition matrix and emission matrix are the updated parameters\n",
    "print(\"Updated transition matrix:\", new_transition_matrix)\n",
    "print(\"Updated emission matrix:\", new_emission_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7.4: Argue with an AI chatbot about hidden Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase: hidden Markov models and the Viterbi and Baum-Welch algorithms are still important and\n",
    "needed in the age of neural large language models\n",
    " try to argue for the opposite position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAT GPT RESPONSE\n",
    "While hidden Markov models (HMMs) and the associated algorithms like Viterbi and Baum-Welch have been fundamental in various applications, arguing for their diminished relevance in the age of neural large language models can be done by highlighting the strengths and advancements of neural models. Here are some points to consider:\n",
    "\n",
    "End-to-End Learning:\n",
    "\n",
    "Neural language models, especially deep learning models like transformers, enable end-to-end learning. They can directly map inputs to outputs without relying on intermediate representations like states in HMMs. This simplifies the modeling process and allows for more direct and seamless learning.\n",
    "Representation Learning:\n",
    "\n",
    "Neural language models excel at learning complex hierarchical representations of data. They automatically learn meaningful features and representations from the input data, eliminating the need for manual feature engineering. In contrast, HMMs may require careful design of features and assumptions about the underlying structure of the data.\n",
    "Scalability:\n",
    "\n",
    "Neural language models can handle large datasets and scale effectively with the increasing availability of computational resources. As data sizes grow, the scalability of neural models becomes a significant advantage compared to HMMs, which may face challenges in handling large datasets efficiently.\n",
    "Task-Specific Architectures:\n",
    "\n",
    "Neural language models can be designed and fine-tuned for specific tasks without the need for a generic model like an HMM. Task-specific architectures can outperform generic models in terms of accuracy and efficiency, providing a more tailored solution for different applications.\n",
    "Complexity of Language:\n",
    "\n",
    "Natural language is highly complex, and neural models have shown an ability to capture intricate patterns and semantics in data. HMMs may struggle to capture the diversity and nuances present in language compared to the expressive power of neural architectures.\n",
    "Training Efficiency:\n",
    "\n",
    "Training neural models often involves end-to-end backpropagation, which can be more efficient than the iterative algorithms like Baum-Welch used in training HMMs. The parallelization capabilities of neural network training contribute to faster convergence on large datasets.\n",
    "Transfer Learning:\n",
    "\n",
    "Pre-trained neural language models support transfer learning, where models trained on a large corpus can be fine-tuned for specific tasks with limited data. This can be advantageous in scenarios where labeled data is scarce, allowing for more effective adaptation to new domains.\n",
    "While HMMs and their associated algorithms have been essential in certain contexts, the capabilities of neural large language models often overshadow their traditional counterparts in terms of flexibility, scalability, and end-to-end learning. It's essential to consider the specific requirements of a task when choosing between these approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase: hidden Markov models and the Viterbi and Baum-Welch algorithms are still important and\n",
    "needed in the age of neural large language models\n",
    "argue for a particular opinion (important/not\n",
    "important) and argue yourself for the opposite position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT RESPONSE\n",
    "\n",
    "\"\n",
    "Arguing for the Importance of Hidden Markov Models (HMMs), Viterbi, and Baum-Welch Algorithms in the Age of Neural Large Language Models:\n",
    "\n",
    "Robustness and Interpretability:\n",
    "\n",
    "HMMs offer a structured probabilistic framework that can provide a level of interpretability lacking in black-box neural models. This is crucial in applications where understanding the underlying process is essential.\n",
    "Limited Data Scenarios:\n",
    "\n",
    "In situations with limited data, which can occur in niche domains or when working with specific languages or dialects, HMMs can be more effective due to their ability to make reasonable assumptions about the underlying structure of the data.\n",
    "Sequential Data Modeling:\n",
    "\n",
    "HMMs are designed to model sequential data, making them well-suited for tasks such as speech recognition, gesture recognition, and natural language processing where the order of observations is crucial. Neural models might struggle with capturing long-range dependencies in sequential data.\n",
    "Training Efficiency:\n",
    "\n",
    "The training process for HMMs, especially with algorithms like Baum-Welch, can be more computationally efficient than training large neural models, especially when dealing with smaller datasets. This efficiency can be crucial in real-time applications.\n",
    "Transfer Learning and Hybrid Models:\n",
    "\n",
    "Combining HMMs with neural models can offer a hybrid approach that benefits from the strengths of both paradigms. For instance, pre-training an HMM on a related task and fine-tuning with a neural model can be a powerful strategy in scenarios where large labeled datasets are not readily available.\n",
    "Arguing for the Diminished Importance of HMMs, Viterbi, and Baum-Welch Algorithms in the Age of Neural Large Language Models:\n",
    "\n",
    "End-to-End Learning:\n",
    "\n",
    "Neural models, especially large language models, are designed for end-to-end learning, eliminating the need for manually engineering features or specifying explicit state transitions as in HMMs. This makes them more adaptable to diverse tasks.\n",
    "Representation Learning:\n",
    "\n",
    "Neural models excel at learning hierarchical representations of data, automatically capturing intricate patterns and dependencies. HMMs, on the other hand, might struggle to capture complex relationships present in large and diverse datasets.\n",
    "Data Abundance:\n",
    "\n",
    "In the age of big data, where massive labeled datasets are available, neural models can leverage this abundance to learn intricate patterns and nuances, potentially outperforming HMMs that might require more manual intervention and domain expertise.\n",
    "Scalability:\n",
    "\n",
    "Neural models can scale to handle vast amounts of data and parameters, making them more suitable for contemporary tasks that involve massive datasets, such as machine translation, sentiment analysis, and question answering.\n",
    "Flexibility:\n",
    "\n",
    "Neural models are inherently more flexible and adaptable to various domains and tasks. Their architectures can be easily modified or fine-tuned for specific applications, providing a level of versatility that might be challenging for HMMs to match.\n",
    "In conclusion, while HMMs and their associated algorithms still hold importance in certain contexts, the rise of neural large language models has reshaped the landscape, emphasizing the need for adaptability, scalability, and end-to-end learning capabilities. The choice between these approaches ultimately depends on the specific requirements and constraints of the given task or application\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for Suitability of Markov Model Assumptions:\n",
    "\n",
    "Simplicity: Assumes simplicity in state transitions, making it easy to model and interpret.\n",
    "\n",
    "Memoryless: Appropriate for certain systems where future states depend only on the current state, simplifying modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Arguments Against Suitability of Markov Model Assumptions:\n",
    "\n",
    "Fixed Order: Assumes fixed order of dependencies, limiting its ability to capture complex, varying relationships.\n",
    "\n",
    "Fixed State: Might struggle in situations where dependencies are influenced by more than just the current state.\n",
    "\n",
    "Limited Context: Fails to consider broader context which can be crucial in certain applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
