{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8.1: HMM modeling of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import hmmlearn, hmmlearn.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = open('hmm_sentences.txt', 'r', encoding='utf-8', errors='ignore')\n",
    "temp = temp_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the \\n at the end of each sentence\n",
    "sentences = []\n",
    "for i in range(len(temp)):\n",
    "    sentences.append(temp[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycrawled_nltktexts=[]\n",
    "for k in range(len(sentences)):    \n",
    "    temp_tokenizedtext=nltk.word_tokenize(sentences[k])\n",
    "    temp_nltktext=nltk.Text(temp_tokenizedtext)\n",
    "    mycrawled_nltktexts.append(temp_nltktext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make all crawled texts lowercase\n",
    "mycrawled_lowercasetexts=[]\n",
    "for k in range(len(mycrawled_nltktexts)):    \n",
    "    temp_lowercasetext=[]\n",
    "    for l in range(len(mycrawled_nltktexts[k])):\n",
    "        lowercaseword=mycrawled_nltktexts[k][l].lower()\n",
    "        temp_lowercasetext.append(lowercaseword)\n",
    "    temp_lowercasetest=nltk.Text(temp_lowercasetext)\n",
    "    mycrawled_lowercasetexts.append(temp_lowercasetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Convert a POS tag for WordNet\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#%% POS tag and lemmatize the loaded texts\n",
    "# Download tagger and wordnet resources if you do not have them already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # Tag the text with POS tags\n",
    "    taggedtext=nltk.pos_tag(nltktexttolemmatize)\n",
    "    # Lemmatize each word text\n",
    "    lemmatizedtext=[]\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize=taggedtext[l][0]\n",
    "        wordnettag=tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag!=-1:\n",
    "            lemmatizedword=lemmatizer.lemmatize(wordtolemmatize,wordnettag)\n",
    "        else:\n",
    "            lemmatizedword=wordtolemmatize\n",
    "        # Store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)\n",
    "\n",
    "mycrawled_lemmatizedtexts=[]\n",
    "for k in range(len(mycrawled_lowercasetexts)):\n",
    "    lemmatizedtext=lemmatizetext(mycrawled_lowercasetexts[k])\n",
    "    lemmatizedtext=nltk.Text(lemmatizedtext)\n",
    "    mycrawled_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Find the vocabulary, in a distributed fashion\n",
    "import numpy\n",
    "myvocabularies=[]\n",
    "myindices_in_vocabularies=[]\n",
    "# Find the vocabulary of each document\n",
    "for k in range(len(mycrawled_lemmatizedtexts)):\n",
    "    # Get unique words and where they occur\n",
    "    temptext=mycrawled_lemmatizedtexts[k]\n",
    "    uniqueresults=numpy.unique(temptext,return_inverse=True)\n",
    "    uniquewords=uniqueresults[0]\n",
    "    wordindices=uniqueresults[1]\n",
    "    # Store the vocabulary and indices of document words in it\n",
    "    myvocabularies.append(uniquewords)\n",
    "    myindices_in_vocabularies.append(wordindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify the vocabularies.\n",
    "# First concatenate all vocabularies\n",
    "tempvocabulary=[]  \n",
    "for k in range(len(mycrawled_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "# Find the unique elements among all vocabularies\n",
    "uniqueresults=numpy.unique(tempvocabulary,return_inverse=True)\n",
    "unifiedvocabulary=uniqueresults[0]\n",
    "wordindices=uniqueresults[1]\n",
    "# Translate previous indices to the unified vocabulary.\n",
    "# Must keep track where each vocabulary started in \n",
    "# the concatenated one.\n",
    "vocabularystart=0\n",
    "myindices_in_unifiedvocabulary=[]\n",
    "for k in range(len(mycrawled_lemmatizedtexts)):\n",
    "    # In order to shift word indices, we must temporarily\n",
    "    # change their data type to a Numpy array\n",
    "    tempindices=numpy.array(myindices_in_vocabularies[k])\n",
    "    tempindices=tempindices+vocabularystart\n",
    "    tempindices=wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart=vocabularystart+len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMMlearn expects the data to be provided \n",
    "# as a (nsamples,1) 2D-array, where the 2nd dimension has just\n",
    "# one element, containing list of indices into a vocabulary, \n",
    "# all documents concatenated together, and separately a \n",
    "# list of lenghts of the individual documents.\n",
    "# Create concatenated index list from previously\n",
    "# crawled and processed documents\n",
    "concatenated_data=[]\n",
    "documentlengths=[]\n",
    "for k in range(len(myindices_in_unifiedvocabulary)):\n",
    "    concatenated_data.extend(myindices_in_unifiedvocabulary[k])\n",
    "    documentlengths.append(len(myindices_in_unifiedvocabulary[k]))\n",
    "concatenated_data=numpy.matrix(concatenated_data).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\.conda\\envs\\r2c-gan\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "         1      -67782.7098             +nan\n",
      "         2      -60292.7918       +7489.9180\n",
      "         3      -60247.3989         +45.3928\n",
      "         4      -60179.1843         +68.2146\n",
      "         5      -60074.1303        +105.0540\n",
      "         6      -59923.7325        +150.3978\n",
      "         7      -59727.5595        +196.1730\n",
      "         8      -59481.0719        +246.4876\n",
      "         9      -59153.9201        +327.1518\n",
      "        10      -58682.3186        +471.6014\n",
      "        11      -57996.1625        +686.1561\n",
      "        12      -57081.3557        +914.8069\n",
      "        13      -55967.5975       +1113.7582\n",
      "        14      -54613.9714       +1353.6261\n",
      "        15      -53179.1325       +1434.8390\n",
      "        16      -52025.4411       +1153.6913\n",
      "        17      -51097.6607        +927.7804\n",
      "        18      -50331.6257        +766.0350\n",
      "        19      -49712.3941        +619.2316\n",
      "        20      -49235.2382        +477.1559\n",
      "        21      -48963.8771        +271.3610\n",
      "        22      -48855.2990        +108.5782\n",
      "        23      -48812.5088         +42.7901\n",
      "        24      -48791.4877         +21.0211\n",
      "        25      -48778.0229         +13.4648\n",
      "        26      -48767.1859         +10.8369\n",
      "        27      -48756.7008         +10.4852\n",
      "        28      -48745.1537         +11.5471\n",
      "        29      -48731.5128         +13.6409\n",
      "        30      -48714.8397         +16.6730\n",
      "        31      -48693.9329         +20.9069\n",
      "        32      -48667.1485         +26.7844\n",
      "        33      -48632.4759         +34.6726\n",
      "        34      -48587.7139         +44.7620\n",
      "        35      -48531.8999         +55.8140\n",
      "        36      -48469.5140         +62.3859\n",
      "        37      -48410.3090         +59.2050\n",
      "        38      -48362.4481         +47.8609\n",
      "        39      -48329.4627         +32.9854\n",
      "        40      -48308.8240         +20.6387\n",
      "        41      -48295.3291         +13.4950\n",
      "        42      -48284.8021         +10.5269\n",
      "        43      -48271.0252         +13.7770\n",
      "        44      -48232.7699         +38.2552\n",
      "        45      -48146.1943         +86.5756\n",
      "        46      -48065.5858         +80.6085\n",
      "        47      -48025.4017         +40.1842\n",
      "        48      -48007.3782         +18.0235\n",
      "        49      -47998.2576          +9.1206\n",
      "        50      -47992.8224          +5.4352\n",
      "        51      -47989.2527          +3.5698\n",
      "        52      -47986.8449          +2.4078\n",
      "        53      -47985.2375          +1.6074\n",
      "        54      -47984.1855          +1.0520\n",
      "        55      -47983.5077          +0.6778\n",
      "        56      -47983.0740          +0.4337\n",
      "        57      -47982.7960          +0.2780\n",
      "        58      -47982.6162          +0.1798\n",
      "        59      -47982.4984          +0.1179\n",
      "        60      -47982.4197          +0.0786\n",
      "        61      -47982.3662          +0.0535\n",
      "        62      -47982.3291          +0.0371\n",
      "        63      -47982.3028          +0.0264\n",
      "        64      -47982.2836          +0.0191\n",
      "        65      -47982.2694          +0.0142\n",
      "        66      -47982.2586          +0.0108\n",
      "        67      -47982.2501          +0.0085\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "myhmm = hmmlearn.hmm.MultinomialHMM(n_components=5, n_iter=100, verbose=True)\n",
    "myhmm_fitted=myhmm.fit(concatenated_data,lengths=documentlengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58298330e-231, 1.03045362e-116, 1.10975106e-047, 1.00000000e+000,\n",
       "       0.00000000e+000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect start, transition, and emission probabilities\n",
    "myhmm_fitted.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+000, 3.16230537e-071, 8.59669574e-047,\n",
       "        0.00000000e+000, 3.21227272e-059, 3.66364105e-177,\n",
       "        2.60590869e-184, 6.43284276e-014, 5.74703531e-156,\n",
       "        4.95392442e-001, 3.11665218e-224, 9.45052505e-151,\n",
       "        3.43117982e-168, 4.55711753e-154, 6.57243970e-176,\n",
       "        8.85430687e-179, 1.57565813e-160, 3.67327542e-155,\n",
       "        4.62774316e-173, 1.82866865e-277, 2.70163457e-173,\n",
       "        2.10805294e-002, 1.77866966e-002, 0.00000000e+000,\n",
       "        3.00397544e-001, 2.79824285e-291, 8.25528004e-169,\n",
       "        1.22468639e-260, 3.55733706e-002, 9.71418258e-157,\n",
       "        4.68337098e-279, 3.72156853e-169, 3.35893403e-171,\n",
       "        0.00000000e+000, 1.97553939e-002, 1.10014013e-001,\n",
       "        6.44963235e-283, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.56868344e-216, 5.41001669e-173,\n",
       "        1.07016626e-008],\n",
       "       [4.26936750e-200, 1.98038784e-023, 3.02420346e-015,\n",
       "        1.45578564e-029, 2.87369777e-040, 6.77135207e-002,\n",
       "        3.89802733e-002, 6.12579321e-051, 3.46282439e-002,\n",
       "        0.00000000e+000, 1.34367203e-027, 1.41349957e-002,\n",
       "        6.22009799e-002, 1.56629505e-002, 6.69230309e-002,\n",
       "        6.47749766e-002, 7.97179338e-003, 4.12110486e-002,\n",
       "        6.74906728e-002, 1.44040886e-003, 3.15468627e-002,\n",
       "        7.85443889e-015, 3.77150861e-011, 0.00000000e+000,\n",
       "        0.00000000e+000, 7.67543967e-003, 1.34371394e-001,\n",
       "        4.49806989e-039, 8.10710796e-009, 3.33498780e-002,\n",
       "        2.99480727e-004, 5.37710240e-002, 1.48040907e-001,\n",
       "        9.84818383e-025, 2.69867771e-006, 0.00000000e+000,\n",
       "        6.32722778e-003, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 5.92089313e-028, 7.23223280e-002,\n",
       "        2.91598560e-002],\n",
       "       [0.00000000e+000, 3.29409872e-037, 7.61937815e-027,\n",
       "        5.56780056e-002, 4.77697399e-083, 1.30485124e-032,\n",
       "        3.50640613e-036, 2.03846657e-001, 3.41940321e-002,\n",
       "        0.00000000e+000, 6.09313245e-002, 1.12457875e-002,\n",
       "        6.88420809e-003, 1.47814149e-002, 1.01163611e-026,\n",
       "        3.13164995e-032, 2.28977212e-026, 3.84146368e-002,\n",
       "        1.01999070e-002, 1.23158871e-002, 3.03722498e-032,\n",
       "        5.19776661e-214, 2.12820457e-209, 0.00000000e+000,\n",
       "        0.00000000e+000, 6.70400976e-002, 1.95108151e-002,\n",
       "        1.19028634e-001, 2.55346612e-207, 3.63128062e-002,\n",
       "        7.02821992e-003, 8.10876529e-003, 2.63012390e-002,\n",
       "        8.74137462e-002, 3.97390137e-207, 0.00000000e+000,\n",
       "        2.95431231e-002, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 8.88666161e-002, 8.44650871e-003,\n",
       "        5.39075688e-002],\n",
       "       [8.66968753e-068, 2.75970020e-081, 9.32502026e-060,\n",
       "        7.40435452e-002, 5.18311833e-093, 1.62456559e-275,\n",
       "        7.98229831e-284, 2.50390454e-001, 3.68675666e-251,\n",
       "        5.22173927e-049, 0.00000000e+000, 2.07087021e-239,\n",
       "        1.10584973e-257, 1.67559869e-253, 1.88423798e-274,\n",
       "        2.99849446e-276, 3.25125096e-295, 3.05648184e-248,\n",
       "        8.42576087e-258, 2.91748161e-002, 2.25452376e-278,\n",
       "        0.00000000e+000, 0.00000000e+000, 2.50069852e-002,\n",
       "        0.00000000e+000, 1.09646012e-001, 9.85872424e-262,\n",
       "        0.00000000e+000, 0.00000000e+000, 1.46106897e-246,\n",
       "        1.79537330e-002, 9.93620475e-259, 3.73518473e-257,\n",
       "        1.03611365e-001, 0.00000000e+000, 7.55085862e-019,\n",
       "        8.14330031e-002, 7.18149319e-002, 4.96933680e-002,\n",
       "        2.27627686e-002, 0.00000000e+000, 3.11254269e-257,\n",
       "        1.64469018e-001],\n",
       "       [4.21953514e-001, 3.27115405e-002, 2.20866293e-001,\n",
       "        0.00000000e+000, 2.30248672e-001, 1.37805465e-003,\n",
       "        5.85099217e-004, 4.41911553e-048, 1.48379825e-249,\n",
       "        0.00000000e+000, 0.00000000e+000, 3.00954923e-241,\n",
       "        0.00000000e+000, 9.82232780e-251, 4.60203733e-004,\n",
       "        1.61539517e-007, 7.37900791e-004, 6.66357104e-246,\n",
       "        0.00000000e+000, 1.31561545e-295, 2.41186780e-005,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 3.36623145e-300, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 8.71363318e-252,\n",
       "        2.69367742e-299, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 9.10344420e-002,\n",
       "        8.90388214e-305, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        4.03027520e-295]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.emissionprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.62103648e-029, 2.95608160e-096, 1.48391854e-035,\n",
       "        3.68909265e-001, 6.31090735e-001],\n",
       "       [3.12310956e-002, 1.01540558e-001, 1.69066158e-001,\n",
       "        1.71529079e-016, 6.98162189e-001],\n",
       "       [2.08789934e-165, 7.77757317e-001, 2.22242683e-001,\n",
       "        2.64112832e-020, 5.40701830e-017],\n",
       "       [0.00000000e+000, 8.51201896e-030, 1.00000000e+000,\n",
       "        1.30860756e-024, 1.44804979e-070],\n",
       "       [4.70325988e-001, 1.80864491e-083, 1.48377176e-043,\n",
       "        5.29674012e-001, 2.00633599e-040]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(myhmm_fitted.transmat_, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the states seem to correspond to meaningful properties of the\n",
    "simplified language since the probability of the states seems to be distributed\n",
    "uniformly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9.1: Inside-outside algorithm.\n",
    "Use the inside-outside algorithm for the Chomsky normal form grammar to calculate the probability of the sentence \"a wise fox can help the friendly insightful cat\".\n",
    "Report your computation and the resulting probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase grammar rules\n",
    "grammar = {\n",
    "    's': [('stmany', 1.0)],\n",
    "    'stmany': [('s1 .', 0.6), ('s1 , but stmany', 0.4)],\n",
    "    's1': [('subj qverb1 qverb2 obj', 1.0)],\n",
    "    'subj': [('article desc noun', 1.0)],\n",
    "    'desc': [('adjective', 0.7), ('adjective desc', 0.3)],\n",
    "    'obj': [('article desc noun', 1.0)],\n",
    "    'qverb1': [('can', 0.2), ('will', 0.5), ('may', 0.3)],\n",
    "    'article': [('a', 0.6), ('the', 0.4)],\n",
    "    'qverb2': [('explain', 0.4), ('help', 0.2), ('answer', 0.4)],\n",
    "    'adjective': [('wise', 0.3), ('friendly', 0.5), ('insightful', 0.2)],\n",
    "    'noun': [('cat', 0.7), ('dog', 0.2), ('fox', 0.1)]\n",
    "}\n",
    "\n",
    "# Sentence\n",
    "sentence = 'a wise fox can help the friendly insightful cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'wise', 'fox', 'can', 'help', 'the', 'friendly', 'insightful', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# Convert sentence to list of words\n",
    "words = sentence.split()\n",
    "print(words)\n",
    "# Initialize inside and outside tables\n",
    "inside = np.zeros((len(words), len(words)))\n",
    "outside = np.zeros((len(words), len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside algorithm\n",
    "for i in range(len(words)):\n",
    "    for j in range(i, -1, -1):\n",
    "        for rule in grammar:\n",
    "            for expansion in grammar[rule]:\n",
    "                if expansion[0] == words[j]:\n",
    "                    inside[j][i] += expansion[1]\n",
    "                elif len(expansion[0].split()) == 2:\n",
    "                    for k in range(j, i):\n",
    "                        inside[j][i] += expansion[1] * inside[j][k] * inside[k+1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outside algorithm\n",
    "outside[0][len(words)-1] = 1\n",
    "for i in range(len(words)):\n",
    "    for j in range(i, -1, -1):\n",
    "        for rule in grammar:\n",
    "            for expansion in grammar[rule]:\n",
    "                if expansion[0] == words[j]:\n",
    "                    outside[j][i] += expansion[1]\n",
    "                elif len(expansion[0].split()) == 2:\n",
    "                    for k in range(j, i):\n",
    "                        outside[j][i] += expansion[1] * outside[j][k] * inside[k+1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(a wise fox can help the friendly insightful cat) = 0.8874280164911058\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities and normalize\n",
    "probabilities = np.zeros((len(words), len(words)))\n",
    "for i in range(len(words)):\n",
    "    for j in range(i, -1, -1):\n",
    "        probabilities[j][i] = inside[j][i] * outside[j][i]\n",
    "    total_prob = sum(probabilities[j][i] for j in range(i + 1) for i in range(len(words)))\n",
    "    probabilities /= total_prob\n",
    "\n",
    "print('P(' + ' '.join(words) + ') = ' + str(probabilities[0][len(words)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
