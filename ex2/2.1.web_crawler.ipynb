{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7d06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get the content of a page using the requests library\n",
    "import requests\n",
    "mywebpage_url='https://www.sis.uta.fi/~tojape/'\n",
    "#mywebpage_url='https://www.tuni.fi/en/'\n",
    "mywebpage_html=requests.get(mywebpage_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98d6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Parse the HTML content using beautifulsoup\n",
    "import bs4\n",
    "mywebpage_parsed=bs4.BeautifulSoup(mywebpage_html.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a0434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get the text content of the page\n",
    "def getpagetext(parsedpage):\n",
    "    # Remove HTML elements that are scripts\n",
    "    scriptelements=parsedpage.find_all('script')\n",
    "    # Concatenate the text content from all table cells\n",
    "    for scriptelement in scriptelements:\n",
    "        # Extract this script element from the page.\n",
    "        # This changes the page given to this function!\n",
    "        scriptelement.extract()\n",
    "    pagetext=parsedpage.get_text()\n",
    "    return(pagetext)\n",
    "\n",
    "mywebpage_text=getpagetext(mywebpage_parsed)\n",
    "# print(mywebpage_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4410949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find HTML elements that are table cells or 'div' cells\n",
    "tablecells=mywebpage_parsed.find_all(['td','div'])\n",
    "# Concatenate the text content from all table or div cells\n",
    "pagetext=''\n",
    "for tablecell in tablecells:\n",
    "    pagetext=pagetext+'\\n'+tablecell.text.strip()\n",
    "# print(pagetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce103f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Find linked pages in Finnish sites, but not PDF or PS files\n",
    "def getpageurls(webpage_parsed):\n",
    "    # Find elements that are hyperlinks\n",
    "    pagelinkelements=webpage_parsed.find_all('a')\n",
    "    pageurls=[];\n",
    "    for pagelink in pagelinkelements:  \n",
    "        pageurl_isok=1\n",
    "        try:\n",
    "            pageurl=pagelink['href']\n",
    "        except:\n",
    "            pageurl_isok=0\n",
    "        if pageurl_isok==1:\n",
    "            # Check that the url does NOT contain these strings\n",
    "            if (pageurl.find('.pdf')!=-1)|(pageurl.find('.ps')!=-1):\n",
    "                pageurl_isok=0\n",
    "            # Check that the url DOES contain these strings\n",
    "            if (pageurl.find('http')==-1)|(pageurl.find('.fi')==-1):\n",
    "                pageurl_isok=0            \n",
    "        if pageurl_isok==1:\n",
    "            pageurls.append(pageurl)\n",
    "    return(pageurls)\n",
    "mywebpage_urls=getpageurls(mywebpage_parsed)\n",
    "# print(mywebpage_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1862efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page:\n",
      "https://www.sis.uta.fi/~tojape/\n",
      "Getting page:\n",
      "https://www.tuni.fi/en\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/about-us/faculty-information-technology-and-communication-sciences\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/about-us/computing-sciences\n",
      "Getting page:\n",
      "http://cs.aalto.fi/en/\n",
      "Getting page:\n",
      "http://www.cis.hut.fi/projects/mi\n",
      "Getting page:\n",
      "http://users.ics.aalto.fi/jtpelto/\n",
      "Getting page:\n",
      "http://research.ics.aalto.fi/coin/\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/study-with-us/computing-sciences-data-science?navref=curated--list\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/study-with-us/computing-sciences-statistical-data-analytics?navref=curated--list\n"
     ]
    }
   ],
   "source": [
    "#%% Basic web crawler\n",
    "def basicwebcrawler(seedpage_url,maxpages):\n",
    "    # Store URLs crawled and their text content\n",
    "    num_pages_crawled=0\n",
    "    crawled_urls=[]\n",
    "    crawled_texts=[]\n",
    "    # Remaining pages to crawl: start from a seed page URL\n",
    "    pagestocrawl=[seedpage_url]\n",
    "    # Process remaining pages until a desired number\n",
    "    # of pages have been found\n",
    "    while (num_pages_crawled<maxpages)&(len(pagestocrawl)>0):\n",
    "        # Retrieve the topmost remaining page and parse it\n",
    "        pagetocrawl_url=pagestocrawl[0]\n",
    "        print('Getting page:')\n",
    "        print(pagetocrawl_url)\n",
    "        pagetocrawl_html=requests.get(pagetocrawl_url)\n",
    "        pagetocrawl_parsed=bs4.BeautifulSoup(pagetocrawl_html.content,'html.parser')\n",
    "        # Get the text and URLs of the page\n",
    "        pagetocrawl_text=getpagetext(pagetocrawl_parsed)\n",
    "        pagetocrawl_urls=getpageurls(pagetocrawl_parsed)\n",
    "        # Store the URL and content of the processed page\n",
    "        num_pages_crawled=num_pages_crawled+1\n",
    "        crawled_urls.append(pagetocrawl_url)\n",
    "        crawled_texts.append(pagetocrawl_text)\n",
    "        # Remove the processed page from remaining pages,\n",
    "        # but add the new URLs\n",
    "        pagestocrawl=pagestocrawl[1:len(pagestocrawl)]\n",
    "        pagestocrawl.extend(pagetocrawl_urls)\n",
    "    return(crawled_urls,crawled_texts)\n",
    "mycrawled_urls_and_texts=basicwebcrawler('https://www.sis.uta.fi/~tojape/',10)\n",
    "mycrawled_urls=mycrawled_urls_and_texts[0]\n",
    "mycrawled_texts=mycrawled_urls_and_texts[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e61cc0",
   "metadata": {},
   "source": [
    "Exercise 2.1: Data acquisition - Building a better web crawler.\n",
    "1.It can crawl the same page multiple times, if a link on a later crawled page points to the already-crawled page\n",
    "-> Solution: Use a list to store the crawled pages, and check if the page is already crawled before crawling it.\n",
    "\n",
    "2.It inserts all links from each page in order as pages to be crawled. If some page contains thousands of links, the crawling will crawl those first and may never get to the links from the next page, especially if the total number of pages are limited\n",
    "-> Solution: Restrict the returned links of each page to a value to avoid crawling too many links from one page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b2354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page:\n",
      "https://www.sis.uta.fi/~tojape/\n",
      "Getting page:\n",
      "https://www.tuni.fi/en\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/about-us/faculty-information-technology-and-communication-sciences\n",
      "Getting page:\n",
      "https://www.tuni.fi/en/about-us/computing-sciences\n",
      "Getting page:\n",
      "http://cs.aalto.fi/en/\n",
      "Getting page:\n",
      "https://research.aalto.fi/\n",
      "Getting page:\n",
      "https://ourblogs.aalto.fi/\n",
      "Getting page:\n",
      "https://ada.aalto.fi/\n",
      "Getting page:\n",
      "https://booking.aalto.fi\n",
      "Getting page:\n",
      "https://www.aalto.fi/en/acris-instructions\n",
      "['https://www.sis.uta.fi/~tojape/', 'https://www.tuni.fi/en', 'https://www.tuni.fi/en/about-us/faculty-information-technology-and-communication-sciences', 'https://www.tuni.fi/en/about-us/computing-sciences', 'http://cs.aalto.fi/en/', 'https://research.aalto.fi/', 'https://ourblogs.aalto.fi/', 'https://ada.aalto.fi/', 'https://booking.aalto.fi', 'https://www.aalto.fi/en/acris-instructions']\n"
     ]
    }
   ],
   "source": [
    "# %% Improve the web crawler by the solution above\n",
    "crawled_urls = []  # Store URLs crawled\n",
    "\n",
    "\n",
    "def improvewebcrawler(seedpage_url, maxpages):\n",
    "    global crawled_urls\n",
    "    # Store URLs crawled and their text content\n",
    "    num_pages_crawled = 0\n",
    "    # crawled_urls = []\n",
    "    crawled_texts = []\n",
    "    # Remaining pages to crawl: start from a seed page URL\n",
    "    pagestocrawl = [seedpage_url]\n",
    "    # Process remaining pages until a desired number\n",
    "    # of pages have been found\n",
    "    while (num_pages_crawled < maxpages) & (len(pagestocrawl) > 0):\n",
    "        # Retrieve the topmost remaining page and parse it\n",
    "        pagetocrawl_url = pagestocrawl[0]\n",
    "        print(\"Getting page:\")\n",
    "        print(pagetocrawl_url)\n",
    "        pagetocrawl_html = requests.get(pagetocrawl_url)\n",
    "        pagetocrawl_parsed = bs4.BeautifulSoup(pagetocrawl_html.content, \"html.parser\")\n",
    "\n",
    "        ### Condition for not overloading the crawled_urls\n",
    "        \n",
    "        if pagetocrawl_url not in crawled_urls:\n",
    "            # Get the text and URLs of the page\n",
    "            pagetocrawl_text = getpagetext(pagetocrawl_parsed)\n",
    "            pagetocrawl_urls = getpageurls(pagetocrawl_parsed)\n",
    "\n",
    "            ### Restrict the number of urls to be crawled to be 5 or less\n",
    "            if len(pagestocrawl) + len(pagetocrawl_urls) > 5:\n",
    "                pagetocrawl_urls = pagetocrawl_urls[0 : 5 - len(pagestocrawl)]\n",
    "\n",
    "            # Store the URL and content of the processed page\n",
    "            num_pages_crawled = num_pages_crawled + 1\n",
    "            crawled_urls.append(pagetocrawl_url)\n",
    "            crawled_texts.append(pagetocrawl_text)\n",
    "            # Remove the processed page from remaining pages,\n",
    "            # but add the new URLs\n",
    "            pagestocrawl = pagestocrawl[1 : len(pagestocrawl)]\n",
    "            pagestocrawl.extend(pagetocrawl_urls)\n",
    "    return (crawled_urls, crawled_texts)\n",
    "mycrawled_urls_and_texts=improvewebcrawler('https://www.sis.uta.fi/~tojape/',10)\n",
    "mycrawled_urls=mycrawled_urls_and_texts[0]\n",
    "mycrawled_texts=mycrawled_urls_and_texts[1]\n",
    "print(crawled_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
